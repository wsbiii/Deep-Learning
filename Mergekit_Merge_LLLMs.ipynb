{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsbiii/Deep-Learning/blob/main/Mergekit_Merge_LLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Large Language Models with mergekit\n",
        "\n",
        "\n",
        "Examples of merge configurations:\n",
        "\n",
        "### TIES-Merging\n",
        "\n",
        "```yaml\n",
        "models:\n",
        "  - model: mistralai/Mistral-7B-v0.1\n",
        "    # no parameters necessary for base model\n",
        "  - model: OpenPipe/mistral-ft-optimized-1218\n",
        "    parameters:\n",
        "      density: 0.5\n",
        "      weight: 0.5\n",
        "  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
        "    parameters:\n",
        "      density: 0.5\n",
        "      weight: 0.3\n",
        "merge_method: ties\n",
        "base_model: mistralai/Mistral-7B-v0.1\n",
        "parameters:\n",
        "  normalize: true\n",
        "dtype: float16\n",
        "```\n",
        "\n",
        "\n",
        "### SLERP\n",
        "\n",
        "```yaml\n",
        "slices:\n",
        "  - sources:\n",
        "      - model: OpenPipe/mistral-ft-optimized-1218\n",
        "        layer_range: [0, 32]\n",
        "      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
        "        layer_range: [0, 32]\n",
        "merge_method: slerp\n",
        "base_model: OpenPipe/mistral-ft-optimized-1218\n",
        "parameters:\n",
        "  t:\n",
        "    - filter: self_attn\n",
        "      value: [0, 0.5, 0.3, 0.7, 1]\n",
        "    - filter: mlp\n",
        "      value: [1, 0.5, 0.7, 0.3, 0]\n",
        "    - value: 0.5\n",
        "dtype: bfloat16\n",
        "```\n",
        "\n",
        "\n",
        "### Passthrough\n",
        "\n",
        "```yaml\n",
        "slices:\n",
        "  - sources:\n",
        "    - model: OpenPipe/mistral-ft-optimized-1218\n",
        "      layer_range: [0, 32]\n",
        "  - sources:\n",
        "    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
        "      layer_range: [24, 32]\n",
        "merge_method: passthrough\n",
        "dtype: bfloat16\n",
        "```\n"
      ],
      "metadata": {
        "id": "o12O0YjJvvLW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPNPie5Eo3EZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b855f52-b49a-46a6-ed14-a8972b6b0aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mergekit'...\n",
            "remote: Enumerating objects: 2157, done.\u001b[K\n",
            "remote: Counting objects: 100% (472/472), done.\u001b[K\n",
            "remote: Compressing objects: 100% (209/209), done.\u001b[K\n",
            "remote: Total 2157 (delta 318), reused 378 (delta 263), pack-reused 1685 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2157/2157), 668.50 KiB | 12.38 MiB/s, done.\n",
            "Resolving deltas: 100% (1462/1462), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m330.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for mergekit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cg123/mergekit.git\n",
        "!cd mergekit && pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "MODEL_NAME = \"Marcoro14-7B-slerp\"\n",
        "yaml_config = \"\"\"\n",
        "slices:\n",
        "  - sources:\n",
        "      - model: AIDC-ai-business/Marcoroni-7B-v3\n",
        "        layer_range: [0, 32]\n",
        "      - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.1\n",
        "        layer_range: [0, 32]\n",
        "merge_method: slerp\n",
        "base_model: AIDC-ai-business/Marcoroni-7B-v3\n",
        "parameters:\n",
        "  t:\n",
        "    - filter: self_attn\n",
        "      value: [0, 0.5, 0.3, 0.7, 1]\n",
        "    - filter: mlp\n",
        "      value: [1, 0.5, 0.7, 0.3, 0]\n",
        "    - value: 0.5\n",
        "dtype: bfloat16\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save config as yaml file\n",
        "with open('config.yaml', 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(yaml_config)"
      ],
      "metadata": {
        "id": "LGd7jlfCpNcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge models\n",
        "!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
      ],
      "metadata": {
        "id": "d5mYzDo1q96y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d58832d-6cc8-40ba-b179-c9b8289f38d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 352, in hf_raise_for_status\n",
            "    raise RepositoryNotFoundError(message, response) from e\n",
            "huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-66c65550-231d7a6e6fbb7bb0059e0c65;b1bfd5b0-b0de-443d-9d44-532b055bfe68)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3/resolve/main/config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mergekit-yaml\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/content/mergekit/mergekit/options.py\", line 82, in wrapper\n",
            "    f(*args, **kwargs)\n",
            "  File \"/content/mergekit/mergekit/scripts/run_yaml.py\", line 47, in main\n",
            "    run_merge(\n",
            "  File \"/content/mergekit/mergekit/merge.py\", line 50, in run_merge\n",
            "    model_arch_info = [\n",
            "  File \"/content/mergekit/mergekit/merge.py\", line 51, in <listcomp>\n",
            "    get_architecture_info(m.config(trust_remote_code=options.trust_remote_code))\n",
            "  File \"/content/mergekit/mergekit/common.py\", line 125, in config\n",
            "    res = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 965, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 425, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: AIDC-ai-business/Marcoroni-7B-v3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU huggingface_hub\n",
        "\n",
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "from jinja2 import Template\n",
        "\n",
        "username = \"wsbreckwoldt\"\n",
        "\n",
        "template_text = \"\"\"\n",
        "---\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- merge\n",
        "- mergekit\n",
        "- lazymergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## ğŸ§© Configuration\n",
        "\n",
        "```yaml\n",
        "{{- yaml_config -}}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "# Create a Jinja template object\n",
        "jinja_template = Template(template_text.strip())\n",
        "\n",
        "# Get list of models from config\n",
        "data = yaml.safe_load(yaml_config)\n",
        "if \"models\" in data:\n",
        "    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\n",
        "elif \"parameters\" in data:\n",
        "    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\n",
        "elif \"slices\" in data:\n",
        "    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\n",
        "else:\n",
        "    raise Exception(\"No models or slices found in yaml config\")\n",
        "\n",
        "# Fill the template\n",
        "content = jinja_template.render(\n",
        "    model_name=MODEL_NAME,\n",
        "    models=models,\n",
        "    yaml_config=yaml_config,\n",
        "    username=username,\n",
        ")\n",
        "\n",
        "# Save the model card\n",
        "card = ModelCard(content)\n",
        "card.save('merge/README.md')"
      ],
      "metadata": {
        "id": "w-RNKev373lI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "username = \"wsbreckwoldt\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get('API_KEY'))\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "api.upload_folder(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path=\"merge\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ik0V0dF55gfU",
        "outputId": "c3cdc1ec-7007-4c6a-a6af-7dbc48379580"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/wsbreckwoldt/Marcoro14-7B-slerp/commit/a1a03b5af74e969476e77ed1450aab15be040466', commit_message='Upload folder using huggingface_hub', commit_description='', oid='a1a03b5af74e969476e77ed1450aab15be040466', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/wsbreckwoldt/Marcoro14-7B-slerp"
      ],
      "metadata": {
        "id": "QcWpmfIS4uTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "eMjgXmoiuR-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54 - Maxime Labonne - [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "> ğŸ—£ï¸ [Large Language Model Course](https://github.com/mlabonne/llm-course)"
      ],
      "metadata": {
        "id": "S8BClMQVuakE"
      }
    }
  ]
}