# -*- coding: utf-8 -*-
"""SAM2: Building Promptable, Object Sergmentation Tools.ipynb

Automatically generated by Colab.

Original notebook file is located at
    https://colab.research.google.com/drive/1g0M1kQRxkHZI9kW0ab2QT-7YC9Dl7bPQ

Segment Anything Model 2 (SAM 2): Meta's real-time, promptable object segmentation tool

https://ai.meta.com/sam2/
"""

! nvidia-smi

!pip install flash_attn -q timm -q
!pip install accelerate -q
!pip install einops -q
!pip install -q supervision
!mkdir -p my_models
!mkdir -p my_models/florence_2

!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P my_models/sam2
!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P my_models/sam2
!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P my_models/sam2
!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P my_models/sam2

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/facebookresearch/segment-anything-2.git
# %cd segment-anything-2
!pip install -e . -q

from transformers import AutoModelForCausalLM, AutoProcessor


model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large-ft",
                                             cache_dir="/content/my_models/Florence_2",
                                             device_map="cuda",
                                             trust_remote_code=True)

processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large-ft",
                                          cache_dir=f"/content/my_models/Florence_2",
                                          trust_remote_code=True)

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pathlib import Path
from PIL import Image
import cv2
import torch
import base64

import numpy as np
import supervision as sv

from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

def find_all_faces(image):
    PROMPT = "<00>"
    task_type = "<00>"

    inputs = processor(text=PROMPT, images=image, return_tensors="pt").to("cuda")

    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=2048,
        do_sample=False,
    )
    text_generations = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

    results = processor.post_process_generation(text_generations, task=task_type, image_size=(image.width, image.height))

    raw_lists=[]

    # Filters human faces
    #From video:
    for bbox, label in zip(results[task_type]['bboxes'], results[task_type]['labels']):
        if label == "human face":
            raw_lists.append(bbox)



      #from Geminini
   # for bbox, label in zip(results.get(task_type, {}).get('bboxes', []), results.get(task_type, {}).get('labels', [])):
   #    if label == "human face":
     #      raw_lists.append(bbox)
   # for bbox, label in zip(results.get(task_type, {}).get('bboxes', []), results.get(task_type, {}).get('labels', [])):
       # if label == "human face":
           # raw_lists.append(bbox)

    # Displays bounding box on all faces
    fig, ax = plt.subplots()
    ax.imshow(image)
    for bbox, label in zip(results[task_type]['bboxes'], results[task_type]['labels']):
        if label == "human face" or label == "person":
            x1, y1, x2, y2 = bbox
            rect_box = patches.Rectangle((x1, y1), x2-x1,y2-y1, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect_box)
            plt.text(x1, y1,label,color='white',fontsize=8,bbox=dict(facecolor='red',alpha=0.5))
    ax.axis('off')
    plt.show()

    return raw_lists

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pathlib import Path
from PIL import Image
import cv2
import torch
import base64

import numpy as np
import supervision as sv

from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

def find_all_faces(image):
    PROMPT = "<00>"
    task_type = "<00>"

    inputs = processor(text=PROMPT, images=image, return_tensors="pt").to("cuda")

    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=2048,
        do_sample=False,
    )
    text_generations = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]


    results = processor.post_process_generation(text_generations, task=task_type, image_size=(image.width, image.height))

    raw_lists=[]

    # Access elements using the string key directly, not the variable
    for bbox, label in zip(results.get('bboxes', []), results.get('labels', [])):
        if label == "human face":
            raw_lists.append(bbox)

    # Displays bounding box on all faces
    fig, ax = plt.subplots()
    ax.imshow(image)

    # Access elements using the string key directly
    for bbox, label in zip(results.get('bboxes', []), results.get('labels', [])):
        if label == "human face" or label == "person":
            x1, y1, x2, y2 = bbox
            rect_box = patches.Rectangle((x1, y1), x2-x1,y2-y1, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect_box)
            plt.text(x1, y1,label,color='white',fontsize=8,bbox=dict(facecolor='red',alpha=0.5))
    ax.axis('off')
    plt.show()

    return raw_lists

def find_main_speakers(image):
    PROMPT = "<CAPTION_TO_PHRASE_GROUNDING> human face (main speaker)"
    task_type = "<CAPTION_TO_PHRASE_GROUNDING>"

    inputs = processor(text=PROMPT, images=image, return_tensors="pt").to("cuda")

    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=2048,
        do_sample=False,
    )
    text_generations = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

    results = processor.post_process_generation(text_generations, task=task_type, image_size=(image.width, image.height))

    fig, ax = plt.subplots()
    ax.imshow(image)
    for bbox, label in zip(results[task_type]['bboxes'], results[task_type]['labels']):
        x1, y1, x2, y2 = bbox
        rect_box = patches.Rectangle((x1, y1), x2-x1,y2-y1,linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect_box)
        plt.text(x1, y1,label,color='white',fontsize=8,bbox=dict(facecolor='red',alpha=0.5))
    ax.axis('off')
    plt.show()

    speaker_face_list = []

    for bbox, label in zip(results[task_type]['bboxes'], results[task_type]['labels']):
        if label == "human face" or label == "person":
            speaker_face_list.append(bbox)

    return speaker_face_list

def is_overlapping(box1,box2,threshold=0.7):
    # Unpack coordinates
    x1_min,y1_min,x1_max, y1_max = box1
    x2_min,y2_min,x2_max, y2_max = box2

    #Calculate the overlap area
    x_overlap = max(0, min(x1_max, x2_max) - max(x1_min, x2_min))
    y_overlap = max(0, min(y1_max, y2_max) - max(y1_min, y2_min))
    overlap_area = x_overlap * y_overlap

    #Calculate the area of each box
    area1 = (x1_max - x1_min) * (y1_max - y1_min)
    area2 = (x2_max - x2_min) * (y2_max - y2_min)

    # Calculate the minimum aarea of the two boxes
    min_area = min(area1,area2)

    # Check if the overlap area is at least the specified percentage of the smaller box's area
    return overlap_area >= threshold * min_area

def filter_boxes(initial_boxes, new_boxes):
    filtered_boxes = []

    for box in initial_boxes:
        if not any(is_overlapping(box,new_box) for new_box in new_boxes):
            filtered_boxes.append(box)

    return filtered_boxes

def find_all_passerbys(image):
    raw_lists = find_all_faces(image)
    speaker_face_list = find_main_speakers(image)

    filtered_faces = filter_boxes(raw_lists, speaker_face_list)

    return filtered_faces

def prepare_mask(mask,image_shape):
    """
    Prepare the mask to match the image dimensions.

    :param mask: The mask to prepare.
    :param image_shape: The shape of the image.
    :return: The prepared mask.
    """
    if mask.ndim == 3:
        mask = mask[0]

    mask = np.resize(mask, (image_shape[0],image_shape[1]))
    return mask

def pixelate_region(image,masks,pixelation_size=10):
    """
    Apply a pixelation effect to the regions of the image specified by the masks.
    :param image: Original image.
    :param masks: List of masks.
    :param pixelation_size: Size of the pixelated regions.
    :return: Pixelated image.
    """
    #Ensure the masks are boolean and have the same spatial dimensions as the image
    masks = masks.astype(bool)

    # Get the dimensions of the image
    height, width = image.shape[:2]

    #Create a copy of the image to modify
    pixelated_image = image.copy()

    # Loop over the image in blocks of pixelation_size
    for y in range(0, height, pixelation_size):
        for x in range(0, width, pixelation_size):
            # Define the block region
            block_y_end = min(y + pixelation_size, height)
            block_x_end = min(x + pixelation_size, width)
            block = image[y:block_y_end, x:block_x_end]

            # Create a combined block mask from all indiviudal masks
            combined_block_mask = np.zeros(block.shape[:2],dtype=bool)
            for mask in masks:
                block_mask = mask[y:block_y_end, x:block_x_end]
                combined_block_mask = np.logical_or(combined_block_mask,block_mask)

    return pixelated_image

def pixelate_all_faces(image_path):
    image = Image.open(image_path).convert("RGB")
    all_results = find_all_passerbys(image)

    if len(all_results) == 0:
        #Conver the image to a NumPy array
        image_array = np.array(image)

        return image_array
    else:
        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        CHECKPOINT = "/contant/my_models/sam2/sam2_hiera_large.pt"
        CONFIG = "sam2_hiera_1.yaml"

        sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)
        predictor = SAM2ImagePredictor(sam2_model)
        predictor.set_image(image)

        masks, scores, logits = predictor.predict(
            box=all_results,
            multimask_output=False
        )

        masks = np.squeeze(masks)

        image_array = np.array(image)
        masks_array = np.array(masks)

        if masks_array.ndim == 2:
            masks_array = np.expand_dims(masks_array, axis=0)

        detections = sv.Detections(
            xyxy=sv.mask_to_xyxy(masks=masks_array),
            mask=masks_array.astype(bool)
        )
        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)
        segmented_image = mask_annotator.annotate(scene=image.copy(), detections=detections)


        fig, ax = plt.subplots()
        ax.inshow(segmented_image)
        ax.axis('off')
        plt.show()

        pixelated_image = pixelate_region(image_array,masks_array)

        return pixelated_image

            # Calculate the average color of the block

test_img = Image.open("/content/syd.jpg").convert("RGB")
find_all_faces(test_img)

SCALE_FACTOR = 1
SOURCE_VIDEO = "/content/test.mp4"
SOURCE_FRAMES = Path(Path(SOURCE_VIDEO).stem)
SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)

print(SOURCE_FRAMES)

# Open the video file
cap = cv2.VideoCapture(SOURCE_VIDEO)
if not cap.isOpened():
    print(f"Error: Could not open video file {SOURCE_VIDEO}")
else:
    print(f"Video {SOURCE_VIDEO} opened successfully")

frame_count = 0

while cap.isOpened():
    # Read a frame from the video
    ret, frame = cap.read()

    if not ret:
        print("End of video")
        break

    # Optionally resize the frame
    frame = cv2.resize(frame, (0,0), fx=SCALE_FACTOR, fy=SCALE_FACTOR)

    # Save the frame
    frame_path = SOURCE_FRAMES / f"{frame_count:05d}.jpeg"
    cv2.imwrite(frame_path.as_posix(), frame)
    print(f"Saved frame {frame_count} to {frame_path}")
    frame_count += 1

cap.release()
print("Video capture released.")

SOURCE_IMAGE = SOURCE_FRAMES / "00000.jpeg"
TARGET_VIDEO = SOURCE_FRAMES.parent / f"{Path(SOURCE_VIDEO).stem}-result.mp4"

print(f"Frames extracted to: {SOURCE_FRAMES}")

def process_images_in_folder(folder_path):
    """
    Processes all images in a given folder by pixelating them.

    Parameters:
        folder_path (str): The path to the folder containing the images.
        pixelation_level (int): The level of pixelation.
    """
    folder_path = Path(folder_path)
    output_folder = folder_path / "pixelated"
    output_folder.mkdir(parents = True, exist_ok=True)

    # Collect and store image paths
    image_paths = sorted(folder_path.glob("*.jpeg"))

    for image_path in image_paths:
        print(f"Processing {image_path}")
        image = cv2.imread(image_path.as_posix())
        if image is None:
            print(f"Error reading {image_path}")
            continue

        pixelated_image = pixelate_all_faces(image_path)

        fig, ax = plt.subplots()
        ax.imshow(pixelated_image)
        ax.axis('off')
        plt.show()

        pil_image = Image.fromarray(pixelated_image)
        output_path = output_folder / image_path.name
        pil_image.save(output_path.as_posix())
        print(f"Saved pixelated image to {output_path}")

def compile_frames_to_videos(source_frames_dir, target_video: str, frame_rate: int=30):
    # Convert source_frames_dir to Path if it's a string
    source_frames_dir = Path(source_frames_dir)

    # Get the list of all frame files, assuming they are sorted numerically
    frame_files = sorted(source_frames_dir.glob("*.jpeg"))

    # Check if any frame files were found
    if not frame_files:
        print(f"Error: No '.jpeg' files found in {source_frames_dir}")
        return  # Exit the function if no files are found

    # Read the first frame to get the frame size
    first_frame = cv2.imread(str(frame_files[0]))
    height, width, layers = first_frame.shape
    frame_size = (width, height)

    # ... (rest of the function remains the same)

# Verify the output directory of process_images_in_folder and adjust if needed
process_images_in_folder("test")
compile_frames_to_videos("/content/segment-anything-2/test", "blur_video.mp4")

"""# References

https://www.youtube.com/watch?v=1ZOYv-b0BLs

https://www.youtube.com/watch?v=IW7jFq3vQbw
"""